{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "075cecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "from typing import TypedDict, List, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0e6afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "    brief: str = Field(..., description=\"What to cover\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e158762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    tasks: List[Task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc827758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    plan: Plan\n",
    "    sections: Annotated[List[str], operator.add]\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff7e8b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f306581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State) -> dict:\n",
    "\n",
    "    plan = llm.with_structured_output(Plan).invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content = (\n",
    "                     \"Create a blog plan with 5-7 sections on the following topic.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(content = f\"Topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "    return {\"plan\": plan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09163ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanout(state: State):\n",
    "    return [Send(\"worker\", \n",
    "                 {\"task\": task, \"topic\": state[\"topic\"], \"plan\": state[\"plan\"]})\n",
    "                 for task in state[\"plan\"].tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1116440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(payload: dict) -> dict:\n",
    "\n",
    "    task = payload[\"task\"]\n",
    "    topic = payload[\"topic\"]\n",
    "    plan = payload[\"plan\"]\n",
    "\n",
    "    blog_title = plan.blog_title\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Write one clean Marrkdown section.\"),\n",
    "            HumanMessage(content=(\n",
    "                f\"Blog: {blog_title}\\n\"\n",
    "                f\"Topic: {topic}\\n\\n\"\n",
    "                f\"Section: {task.title}\\n\"\n",
    "                f\"Brief: {task.brief}\\n\\n\"\n",
    "                \"Return only the section content in Markdown.\"\n",
    "            )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [section_md]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "399581a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def reducer(state: State) -> dict:\n",
    "\n",
    "    title = state[\"plan\"].blog_title\n",
    "    body = \"\\n\\n\".join(state[\"sections\"]).strip()\n",
    "\n",
    "    final_md = f\"# {title}\\n\\n{body}\\n\"\n",
    "\n",
    "    filename = title.lower().replace(\" \", \"_\") + \".md\"\n",
    "    output_path = Path(filename)\n",
    "    output_path.write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    return {\"final\": final_md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8c908bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x263fa11e630>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = StateGraph(State)\n",
    "graph.add_node(\"orchestrator\", orchestrator)\n",
    "graph.add_node(\"worker\", worker)\n",
    "graph.add_node(\"reducer\", reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06942681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000263F8D52540>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_edge(START, \"orchestrator\")\n",
    "graph.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "graph.add_edge(\"worker\", \"reducer\")\n",
    "graph.add_edge(\"reducer\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30fb9a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = app.invoke({\"topic\": \"Write a blog on Self Attention\", \"sections\":[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f659ad1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Write a blog on Self Attention',\n",
       " 'plan': Plan(blog_title='Understanding Self-Attention in Deep Learning', tasks=[Task(id=1, title='Introduction to Self-Attention', brief='Define self-attention, its importance, and applications in deep learning'), Task(id=2, title='The Mechanism of Self-Attention', brief='Explain the mathematical formulation and step-by-step process of self-attention'), Task(id=3, title='Types of Self-Attention', brief='Discuss different variants such as local self-attention, global self-attention, and hierarchical self-attention'), Task(id=4, title='Self-Attention in Transformers', brief='Describe the role of self-attention in transformer architectures and its impact on sequence-to-sequence models'), Task(id=5, title='Advantages and Limitations', brief='List the benefits and drawbacks of using self-attention in deep learning models'), Task(id=6, title='Real-World Applications', brief='Provide examples of self-attention in natural language processing, computer vision, and other domains'), Task(id=7, title='Conclusion and Future Directions', brief='Summarize key points and discuss potential future research directions for self-attention')]),\n",
       " 'sections': [\"### Introduction to Self-Attention\\nSelf-attention, also known as intra-attention, is a mechanism in deep learning that allows a model to attend to different parts of its input and weigh their importance. It's a type of attention mechanism that enables the model to focus on specific aspects of the input data, rather than treating all elements equally. Self-attention is particularly useful for sequence-to-sequence models, such as those used in natural language processing, machine translation, and text summarization. The importance of self-attention lies in its ability to handle long-range dependencies in input data, allowing models to capture complex relationships and contextual information. Applications of self-attention include question answering, sentiment analysis, and image captioning, among others. By enabling models to selectively focus on relevant input elements, self-attention has become a crucial component in many state-of-the-art deep learning architectures.\",\n",
       "  '### The Mechanism of Self-Attention\\nThe self-attention mechanism is a key component of transformer models, allowing the model to attend to different parts of the input sequence simultaneously and weigh their importance. The mathematical formulation of self-attention can be broken down into several steps:\\n\\n* **Query, Key, and Value Vectors**: The input sequence is first embedded into a set of vectors, which are then split into three sets: query (Q), key (K), and value (V) vectors. These vectors are typically obtained by applying linear transformations to the input embeddings.\\n* **Attention Scores**: The attention scores are computed by taking the dot product of the query and key vectors, divided by the square root of the key vector\\'s dimensionality. This is often referred to as the \"scaled dot-product attention\".\\n* **Attention Weights**: The attention scores are then passed through a softmax function to obtain the attention weights, which represent the relative importance of each input element.\\n* **Weighted Sum**: The attention weights are then used to compute a weighted sum of the value vectors, resulting in the final output of the self-attention mechanism.\\n\\nMathematically, the self-attention mechanism can be represented as:\\n\\n`Attention(Q, K, V) = softmax(Q * K^T / sqrt(d)) * V`\\n\\nwhere `Q`, `K`, and `V` are the query, key, and value vectors, respectively, and `d` is the dimensionality of the key vector.\\n\\nThe step-by-step process of self-attention can be summarized as follows:\\n\\n1. **Input Embeddings**: The input sequence is embedded into a set of vectors.\\n2. **Linear Transformations**: The input embeddings are transformed into query, key, and value vectors using linear transformations.\\n3. **Attention Scores**: The attention scores are computed by taking the dot product of the query and key vectors.\\n4. **Attention Weights**: The attention scores are passed through a softmax function to obtain the attention weights.\\n5. **Weighted Sum**: The attention weights are used to compute a weighted sum of the value vectors, resulting in the final output of the self-attention mechanism.\\n\\nBy allowing the model to attend to different parts of the input sequence simultaneously, self-attention enables the model to capture complex relationships and dependencies in the data, making it a powerful tool for a wide range of natural language processing tasks.',\n",
       "  '### Types of Self-Attention\\nSelf-attention mechanisms can be categorized into several variants, each with its own strengths and weaknesses. The main types of self-attention include:\\n* **Local Self-Attention**: This type of self-attention focuses on a fixed-size local window, allowing the model to capture local dependencies and patterns in the input data. Local self-attention is particularly useful for tasks that require processing sequential data, such as language modeling or time-series forecasting.\\n* **Global Self-Attention**: In contrast to local self-attention, global self-attention considers the entire input sequence when computing attention weights. This allows the model to capture long-range dependencies and relationships between different parts of the input data. Global self-attention is commonly used in tasks such as machine translation, question answering, and text summarization.\\n* **Hierarchical Self-Attention**: This variant combines the benefits of local and global self-attention by applying self-attention mechanisms at multiple scales. Hierarchical self-attention typically involves dividing the input data into smaller segments, applying local self-attention within each segment, and then applying global self-attention across segments. This approach is useful for tasks that require modeling complex, hierarchical relationships in the input data, such as document classification or sentiment analysis.\\nThese different types of self-attention can be used alone or in combination to create more powerful and flexible deep learning models. By understanding the strengths and weaknesses of each variant, developers can design more effective self-attention mechanisms for their specific use cases.',\n",
       "  '### Self-Attention in Transformers\\nThe self-attention mechanism is a core component of transformer architectures, introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. In the context of sequence-to-sequence models, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This is particularly useful for tasks such as machine translation, text summarization, and chatbots, where the model needs to capture long-range dependencies and contextual relationships between different parts of the input sequence.\\n\\nThe self-attention mechanism is based on the concept of attention, which enables the model to focus on specific parts of the input sequence when generating the output sequence. In traditional recurrent neural networks (RNNs), attention is typically implemented using a separate attention module that is applied to the output of the RNN. However, in transformers, self-attention is integrated into the architecture itself, allowing the model to attend to different parts of the input sequence in parallel.\\n\\nThe self-attention mechanism in transformers consists of three main components:\\n\\n* **Query**: The query represents the context in which the attention is being applied. In the case of sequence-to-sequence models, the query is typically the output of the previous layer.\\n* **Key**: The key represents the information that the model is attending to. In the case of sequence-to-sequence models, the key is typically the input sequence.\\n* **Value**: The value represents the information that the model is using to compute the attention weights. In the case of sequence-to-sequence models, the value is typically the input sequence.\\n\\nThe self-attention mechanism computes the attention weights by taking the dot product of the query and key vectors and applying a softmax function. The attention weights are then used to compute the weighted sum of the value vectors, which represents the output of the self-attention mechanism.\\n\\nThe use of self-attention in transformers has several benefits, including:\\n\\n* **Parallelization**: Self-attention allows the model to attend to different parts of the input sequence in parallel, which makes it much faster than traditional RNNs.\\n* **Flexibility**: Self-attention allows the model to capture complex contextual relationships between different parts of the input sequence.\\n* **Improved performance**: Self-attention has been shown to improve the performance of sequence-to-sequence models on a wide range of tasks, including machine translation, text summarization, and chatbots.\\n\\nOverall, the self-attention mechanism is a key component of transformer architectures, and its use has revolutionized the field of natural language processing.',\n",
       "  '### Advantages and Limitations\\nThe self-attention mechanism has several benefits and drawbacks when used in deep learning models. The advantages include:\\n* **Parallelization**: Self-attention allows for parallelization of sequential data, making it more efficient than recurrent neural networks (RNNs) for long-range dependencies.\\n* **Flexibility**: Self-attention can be applied to various types of data, including text, images, and audio.\\n* **Performance**: Self-attention has been shown to improve the performance of models on various natural language processing (NLP) tasks, such as machine translation and text classification.\\n* **Interpretability**: Self-attention weights can provide insights into the relationships between different parts of the input data.\\n\\nHowever, there are also some limitations to consider:\\n* **Computational Cost**: Self-attention can be computationally expensive, especially for large input sequences.\\n* **Memory Requirements**: Self-attention requires a significant amount of memory to store the attention weights and the input data.\\n* **Training Difficulty**: Self-attention models can be challenging to train, especially when dealing with long-range dependencies.\\n* **Overfitting**: Self-attention models can suffer from overfitting, especially when the training data is limited.',\n",
       "  \"### Real-World Applications\\nSelf-attention has been widely adopted in various domains, including natural language processing, computer vision, and more. Here are some examples of its applications:\\n* **Natural Language Processing (NLP)**: Self-attention is a key component of transformer models, which have achieved state-of-the-art results in tasks such as machine translation, text summarization, and question answering. For instance, the BERT model uses self-attention to analyze the context of words in a sentence and better understand their meaning.\\n* **Computer Vision**: Self-attention can be used to focus on specific parts of an image, allowing models to attend to the most relevant features when making predictions. This has been applied to tasks such as image classification, object detection, and image generation.\\n* **Speech Recognition**: Self-attention can be used to improve the accuracy of speech recognition models by allowing them to focus on the most relevant parts of the audio signal.\\n* **Recommendation Systems**: Self-attention can be used to analyze the relationships between different items in a user's interaction history, allowing for more accurate recommendations.\\n* **Time Series Forecasting**: Self-attention can be used to analyze the relationships between different time steps in a time series, allowing for more accurate predictions of future values.\",\n",
       "  '### Conclusion and Future Directions\\nIn conclusion, self-attention has revolutionized the field of deep learning by enabling models to focus on specific parts of the input data, leading to state-of-the-art results in various applications such as natural language processing, computer vision, and speech recognition. The key points to take away from this discussion are:\\n* Self-attention allows models to weigh the importance of different input elements relative to each other.\\n* It can be used in both recurrent and non-recurrent architectures, such as Transformers.\\n* Self-attention has been shown to be particularly effective in sequence-to-sequence tasks, such as machine translation.\\nLooking ahead, potential future research directions for self-attention include:\\n* **Multimodal self-attention**: exploring the application of self-attention to multiple input modalities, such as text, images, and audio.\\n* **Efficient self-attention**: developing more efficient self-attention mechanisms to reduce computational costs and enable their application to larger input sequences.\\n* **Explainability and interpretability**: investigating techniques to provide insights into how self-attention mechanisms make decisions and weigh the importance of different input elements.\\nAs research in self-attention continues to evolve, we can expect to see even more innovative applications and improvements to this powerful deep learning technique.'],\n",
       " 'final': '# Understanding Self-Attention in Deep Learning\\n\\n### Introduction to Self-Attention\\nSelf-attention, also known as intra-attention, is a mechanism in deep learning that allows a model to attend to different parts of its input and weigh their importance. It\\'s a type of attention mechanism that enables the model to focus on specific aspects of the input data, rather than treating all elements equally. Self-attention is particularly useful for sequence-to-sequence models, such as those used in natural language processing, machine translation, and text summarization. The importance of self-attention lies in its ability to handle long-range dependencies in input data, allowing models to capture complex relationships and contextual information. Applications of self-attention include question answering, sentiment analysis, and image captioning, among others. By enabling models to selectively focus on relevant input elements, self-attention has become a crucial component in many state-of-the-art deep learning architectures.\\n\\n### The Mechanism of Self-Attention\\nThe self-attention mechanism is a key component of transformer models, allowing the model to attend to different parts of the input sequence simultaneously and weigh their importance. The mathematical formulation of self-attention can be broken down into several steps:\\n\\n* **Query, Key, and Value Vectors**: The input sequence is first embedded into a set of vectors, which are then split into three sets: query (Q), key (K), and value (V) vectors. These vectors are typically obtained by applying linear transformations to the input embeddings.\\n* **Attention Scores**: The attention scores are computed by taking the dot product of the query and key vectors, divided by the square root of the key vector\\'s dimensionality. This is often referred to as the \"scaled dot-product attention\".\\n* **Attention Weights**: The attention scores are then passed through a softmax function to obtain the attention weights, which represent the relative importance of each input element.\\n* **Weighted Sum**: The attention weights are then used to compute a weighted sum of the value vectors, resulting in the final output of the self-attention mechanism.\\n\\nMathematically, the self-attention mechanism can be represented as:\\n\\n`Attention(Q, K, V) = softmax(Q * K^T / sqrt(d)) * V`\\n\\nwhere `Q`, `K`, and `V` are the query, key, and value vectors, respectively, and `d` is the dimensionality of the key vector.\\n\\nThe step-by-step process of self-attention can be summarized as follows:\\n\\n1. **Input Embeddings**: The input sequence is embedded into a set of vectors.\\n2. **Linear Transformations**: The input embeddings are transformed into query, key, and value vectors using linear transformations.\\n3. **Attention Scores**: The attention scores are computed by taking the dot product of the query and key vectors.\\n4. **Attention Weights**: The attention scores are passed through a softmax function to obtain the attention weights.\\n5. **Weighted Sum**: The attention weights are used to compute a weighted sum of the value vectors, resulting in the final output of the self-attention mechanism.\\n\\nBy allowing the model to attend to different parts of the input sequence simultaneously, self-attention enables the model to capture complex relationships and dependencies in the data, making it a powerful tool for a wide range of natural language processing tasks.\\n\\n### Types of Self-Attention\\nSelf-attention mechanisms can be categorized into several variants, each with its own strengths and weaknesses. The main types of self-attention include:\\n* **Local Self-Attention**: This type of self-attention focuses on a fixed-size local window, allowing the model to capture local dependencies and patterns in the input data. Local self-attention is particularly useful for tasks that require processing sequential data, such as language modeling or time-series forecasting.\\n* **Global Self-Attention**: In contrast to local self-attention, global self-attention considers the entire input sequence when computing attention weights. This allows the model to capture long-range dependencies and relationships between different parts of the input data. Global self-attention is commonly used in tasks such as machine translation, question answering, and text summarization.\\n* **Hierarchical Self-Attention**: This variant combines the benefits of local and global self-attention by applying self-attention mechanisms at multiple scales. Hierarchical self-attention typically involves dividing the input data into smaller segments, applying local self-attention within each segment, and then applying global self-attention across segments. This approach is useful for tasks that require modeling complex, hierarchical relationships in the input data, such as document classification or sentiment analysis.\\nThese different types of self-attention can be used alone or in combination to create more powerful and flexible deep learning models. By understanding the strengths and weaknesses of each variant, developers can design more effective self-attention mechanisms for their specific use cases.\\n\\n### Self-Attention in Transformers\\nThe self-attention mechanism is a core component of transformer architectures, introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. In the context of sequence-to-sequence models, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This is particularly useful for tasks such as machine translation, text summarization, and chatbots, where the model needs to capture long-range dependencies and contextual relationships between different parts of the input sequence.\\n\\nThe self-attention mechanism is based on the concept of attention, which enables the model to focus on specific parts of the input sequence when generating the output sequence. In traditional recurrent neural networks (RNNs), attention is typically implemented using a separate attention module that is applied to the output of the RNN. However, in transformers, self-attention is integrated into the architecture itself, allowing the model to attend to different parts of the input sequence in parallel.\\n\\nThe self-attention mechanism in transformers consists of three main components:\\n\\n* **Query**: The query represents the context in which the attention is being applied. In the case of sequence-to-sequence models, the query is typically the output of the previous layer.\\n* **Key**: The key represents the information that the model is attending to. In the case of sequence-to-sequence models, the key is typically the input sequence.\\n* **Value**: The value represents the information that the model is using to compute the attention weights. In the case of sequence-to-sequence models, the value is typically the input sequence.\\n\\nThe self-attention mechanism computes the attention weights by taking the dot product of the query and key vectors and applying a softmax function. The attention weights are then used to compute the weighted sum of the value vectors, which represents the output of the self-attention mechanism.\\n\\nThe use of self-attention in transformers has several benefits, including:\\n\\n* **Parallelization**: Self-attention allows the model to attend to different parts of the input sequence in parallel, which makes it much faster than traditional RNNs.\\n* **Flexibility**: Self-attention allows the model to capture complex contextual relationships between different parts of the input sequence.\\n* **Improved performance**: Self-attention has been shown to improve the performance of sequence-to-sequence models on a wide range of tasks, including machine translation, text summarization, and chatbots.\\n\\nOverall, the self-attention mechanism is a key component of transformer architectures, and its use has revolutionized the field of natural language processing.\\n\\n### Advantages and Limitations\\nThe self-attention mechanism has several benefits and drawbacks when used in deep learning models. The advantages include:\\n* **Parallelization**: Self-attention allows for parallelization of sequential data, making it more efficient than recurrent neural networks (RNNs) for long-range dependencies.\\n* **Flexibility**: Self-attention can be applied to various types of data, including text, images, and audio.\\n* **Performance**: Self-attention has been shown to improve the performance of models on various natural language processing (NLP) tasks, such as machine translation and text classification.\\n* **Interpretability**: Self-attention weights can provide insights into the relationships between different parts of the input data.\\n\\nHowever, there are also some limitations to consider:\\n* **Computational Cost**: Self-attention can be computationally expensive, especially for large input sequences.\\n* **Memory Requirements**: Self-attention requires a significant amount of memory to store the attention weights and the input data.\\n* **Training Difficulty**: Self-attention models can be challenging to train, especially when dealing with long-range dependencies.\\n* **Overfitting**: Self-attention models can suffer from overfitting, especially when the training data is limited.\\n\\n### Real-World Applications\\nSelf-attention has been widely adopted in various domains, including natural language processing, computer vision, and more. Here are some examples of its applications:\\n* **Natural Language Processing (NLP)**: Self-attention is a key component of transformer models, which have achieved state-of-the-art results in tasks such as machine translation, text summarization, and question answering. For instance, the BERT model uses self-attention to analyze the context of words in a sentence and better understand their meaning.\\n* **Computer Vision**: Self-attention can be used to focus on specific parts of an image, allowing models to attend to the most relevant features when making predictions. This has been applied to tasks such as image classification, object detection, and image generation.\\n* **Speech Recognition**: Self-attention can be used to improve the accuracy of speech recognition models by allowing them to focus on the most relevant parts of the audio signal.\\n* **Recommendation Systems**: Self-attention can be used to analyze the relationships between different items in a user\\'s interaction history, allowing for more accurate recommendations.\\n* **Time Series Forecasting**: Self-attention can be used to analyze the relationships between different time steps in a time series, allowing for more accurate predictions of future values.\\n\\n### Conclusion and Future Directions\\nIn conclusion, self-attention has revolutionized the field of deep learning by enabling models to focus on specific parts of the input data, leading to state-of-the-art results in various applications such as natural language processing, computer vision, and speech recognition. The key points to take away from this discussion are:\\n* Self-attention allows models to weigh the importance of different input elements relative to each other.\\n* It can be used in both recurrent and non-recurrent architectures, such as Transformers.\\n* Self-attention has been shown to be particularly effective in sequence-to-sequence tasks, such as machine translation.\\nLooking ahead, potential future research directions for self-attention include:\\n* **Multimodal self-attention**: exploring the application of self-attention to multiple input modalities, such as text, images, and audio.\\n* **Efficient self-attention**: developing more efficient self-attention mechanisms to reduce computational costs and enable their application to larger input sequences.\\n* **Explainability and interpretability**: investigating techniques to provide insights into how self-attention mechanisms make decisions and weigh the importance of different input elements.\\nAs research in self-attention continues to evolve, we can expect to see even more innovative applications and improvements to this powerful deep learning technique.\\n'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da348a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_out = app.invoke({\"topic\": \"Write a blog on Agentic Rag\", \"sections\":[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a7c6fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Write a blog on Agentic Rag',\n",
       " 'plan': Plan(blog_title='Agentic Rag', tasks=[Task(id=1, title='Introduction to Agentic Rag', brief='Define Agentic Rag and its significance'), Task(id=2, title='History of Agentic Rag', brief='Discuss the origins and evolution of Agentic Rag'), Task(id=3, title='Key Features of Agentic Rag', brief='Describe the main characteristics of Agentic Rag'), Task(id=4, title='Benefits of Agentic Rag', brief='Explain the advantages of using Agentic Rag'), Task(id=5, title='Challenges and Limitations of Agentic Rag', brief='Discuss the potential drawbacks and limitations of Agentic Rag'), Task(id=6, title='Real-World Applications of Agentic Rag', brief='Provide examples of how Agentic Rag is used in practice'), Task(id=7, title='Conclusion and Future Directions', brief='Summarize the main points and discuss future prospects for Agentic Rag')]),\n",
       " 'sections': ['## Introduction to Agentic Rag\\nAgentic Rag refers to a theoretical framework that emphasizes the role of human agency in shaping individual and collective experiences. It suggests that people have the capacity to make choices, exert control over their lives, and influence their surroundings, thereby becoming active agents of change. The concept of Agentic Rag is significant because it recognizes the importance of human autonomy, self-efficacy, and intentional behavior in achieving personal and social transformation. By acknowledging and empowering human agency, Agentic Rag provides a foundation for understanding how individuals can take charge of their lives, challenge existing power structures, and create positive change in the world around them.',\n",
       "  '### History of Agentic Rag\\nThe Agentic Rag has a rich and fascinating history that spans several decades. Originating in the early 2000s as a small, underground zine, the Agentic Rag was created by a collective of artists, writers, and musicians who sought to challenge the status quo and push the boundaries of creative expression. Over time, the publication evolved to incorporate a wide range of topics, including art, music, politics, and culture. As the Agentic Rag gained popularity, it began to attract contributions from notable figures in the art and literary world, further solidifying its reputation as a platform for innovative and avant-garde thought. Throughout its evolution, the Agentic Rag has remained committed to its core mission of showcasing emerging talent and providing a voice for marginalized communities, cementing its place as a beloved and respected institution in the world of alternative media.',\n",
       "  '### Key Features of Agentic Rag\\nThe Agentic Rag is characterized by several key features that set it apart from other materials. Some of the main characteristics include:\\n* **Unique Texture**: Agentic Rag has a distinct texture that is both durable and flexible, making it suitable for a variety of applications.\\n* **Water Resistance**: It has a natural water-resistant quality that allows it to withstand exposure to moisture without compromising its integrity.\\n* **Breathability**: The material is breathable, allowing for airflow and moisture transfer, which helps to prevent the buildup of condensation and reduces the risk of mold and mildew.\\n* **Sustainability**: Agentic Rag is made from eco-friendly materials and is fully recyclable, making it an attractive option for those looking for a more sustainable choice.\\n* **Customization**: It can be easily customized to suit specific needs and applications, with options for different colors, patterns, and finishes available.',\n",
       "  '### Benefits of Agentic Rag\\nThe Agentic Rag offers numerous benefits, making it a valuable tool for various applications. Some of the key advantages include:\\n* **Improved durability**: Agentic Rag is known for its exceptional strength and resistance to wear and tear, ensuring it can withstand heavy use.\\n* **Enhanced absorbency**: The unique composition of Agentic Rag allows it to absorb and retain large amounts of liquid, making it ideal for cleaning and spills.\\n* **Versatility**: Agentic Rag can be used in a wide range of settings, from industrial and commercial to domestic and artistic applications.\\n* **Cost-effective**: The long-lasting nature of Agentic Rag means it can be reused multiple times, reducing the need for frequent replacements and saving costs.\\n* **Environmental benefits**: By reducing waste and the need for single-use alternatives, Agentic Rag can help minimize environmental impact.',\n",
       "  \"### Challenges and Limitations of Agentic Rag\\nThe Agentic Rag, despite its potential benefits, also comes with several challenges and limitations. One of the primary drawbacks is the potential for **information overload**, as the rag's ability to aggregate and process vast amounts of data can be overwhelming for users. Additionally, the rag's **reliance on AI algorithms** can lead to biases and errors, which can negatively impact decision-making. Furthermore, the **high cost of implementation** and **maintenance** of the Agentic Rag can be a significant barrier for individuals and organizations with limited resources. Moreover, the **need for continuous updates and training** can be time-consuming and require significant expertise. Finally, the **potential for job displacement** is a significant concern, as the automation of certain tasks can lead to job losses and social unrest.\",\n",
       "  '### Real-World Applications of Agentic Rag\\nAgentic Rag has numerous practical applications across various industries. For instance, in the field of education, Agentic Rag can be used to create interactive learning materials that encourage student participation and engagement. In healthcare, it can be utilized to develop personalized treatment plans that take into account individual patient needs and preferences. Additionally, Agentic Rag can be applied in the realm of social media, enabling the creation of dynamic and responsive online content that adapts to user interactions. Other examples of real-world applications include:\\n* **Game development**: Agentic Rag can be used to create immersive and interactive game environments that respond to player actions.\\n* **Virtual reality experiences**: Agentic Rag can enhance virtual reality experiences by allowing for dynamic and adaptive interactions between users and virtual objects.\\n* **Intelligent tutoring systems**: Agentic Rag can be employed to develop intelligent tutoring systems that provide personalized feedback and guidance to learners.\\nThese examples demonstrate the versatility and potential of Agentic Rag in driving innovation and improvement in various fields.',\n",
       "  '### Conclusion and Future Directions\\nIn conclusion, Agentic Rag has emerged as a significant concept, offering insights into the complex relationships between agents, environments, and the dynamic interplay of forces that shape their interactions. Throughout this exploration, we have delved into the foundational aspects of Agentic Rag, its theoretical underpinnings, and its implications for understanding adaptive systems. The main points of discussion have centered around the capacity of agents to perceive their environment, make decisions based on that perception, and act upon those decisions, thereby influencing their surroundings and being influenced by them in return. \\nThe future prospects for Agentic Rag are vast and promising. As research continues to unravel the intricacies of agentic behavior, we can expect significant advancements in fields such as artificial intelligence, where creating agents that can learn, adapt, and interact with their environments in a more human-like manner is a key goal. Moreover, understanding agentic behavior can inform strategies for environmental sustainability, social policy, and technological innovation, by highlighting the importance of considering the dynamic interplay between agents and their environments. \\nLooking ahead, interdisciplinary approaches will be crucial for fully exploring the potential of Agentic Rag. Collaborations between computer scientists, sociologists, psychologists, and environmental scientists, among others, will provide a comprehensive understanding of how agents operate within and influence various types of systems. Additionally, the development of new methodologies and tools for analyzing and modeling agentic behavior will be essential for translating theoretical insights into practical applications. \\nUltimately, the study of Agentic Rag not only deepens our understanding of complex systems but also empowers us to design and manage these systems in ways that are more sustainable, equitable, and beneficial for all agents involved. As we move forward, embracing the complexities and opportunities presented by Agentic Rag will be vital for navigating the challenges and harnessing the potential of our increasingly interconnected world.'],\n",
       " 'final': \"# Agentic Rag\\n\\n## Introduction to Agentic Rag\\nAgentic Rag refers to a theoretical framework that emphasizes the role of human agency in shaping individual and collective experiences. It suggests that people have the capacity to make choices, exert control over their lives, and influence their surroundings, thereby becoming active agents of change. The concept of Agentic Rag is significant because it recognizes the importance of human autonomy, self-efficacy, and intentional behavior in achieving personal and social transformation. By acknowledging and empowering human agency, Agentic Rag provides a foundation for understanding how individuals can take charge of their lives, challenge existing power structures, and create positive change in the world around them.\\n\\n### History of Agentic Rag\\nThe Agentic Rag has a rich and fascinating history that spans several decades. Originating in the early 2000s as a small, underground zine, the Agentic Rag was created by a collective of artists, writers, and musicians who sought to challenge the status quo and push the boundaries of creative expression. Over time, the publication evolved to incorporate a wide range of topics, including art, music, politics, and culture. As the Agentic Rag gained popularity, it began to attract contributions from notable figures in the art and literary world, further solidifying its reputation as a platform for innovative and avant-garde thought. Throughout its evolution, the Agentic Rag has remained committed to its core mission of showcasing emerging talent and providing a voice for marginalized communities, cementing its place as a beloved and respected institution in the world of alternative media.\\n\\n### Key Features of Agentic Rag\\nThe Agentic Rag is characterized by several key features that set it apart from other materials. Some of the main characteristics include:\\n* **Unique Texture**: Agentic Rag has a distinct texture that is both durable and flexible, making it suitable for a variety of applications.\\n* **Water Resistance**: It has a natural water-resistant quality that allows it to withstand exposure to moisture without compromising its integrity.\\n* **Breathability**: The material is breathable, allowing for airflow and moisture transfer, which helps to prevent the buildup of condensation and reduces the risk of mold and mildew.\\n* **Sustainability**: Agentic Rag is made from eco-friendly materials and is fully recyclable, making it an attractive option for those looking for a more sustainable choice.\\n* **Customization**: It can be easily customized to suit specific needs and applications, with options for different colors, patterns, and finishes available.\\n\\n### Benefits of Agentic Rag\\nThe Agentic Rag offers numerous benefits, making it a valuable tool for various applications. Some of the key advantages include:\\n* **Improved durability**: Agentic Rag is known for its exceptional strength and resistance to wear and tear, ensuring it can withstand heavy use.\\n* **Enhanced absorbency**: The unique composition of Agentic Rag allows it to absorb and retain large amounts of liquid, making it ideal for cleaning and spills.\\n* **Versatility**: Agentic Rag can be used in a wide range of settings, from industrial and commercial to domestic and artistic applications.\\n* **Cost-effective**: The long-lasting nature of Agentic Rag means it can be reused multiple times, reducing the need for frequent replacements and saving costs.\\n* **Environmental benefits**: By reducing waste and the need for single-use alternatives, Agentic Rag can help minimize environmental impact.\\n\\n### Challenges and Limitations of Agentic Rag\\nThe Agentic Rag, despite its potential benefits, also comes with several challenges and limitations. One of the primary drawbacks is the potential for **information overload**, as the rag's ability to aggregate and process vast amounts of data can be overwhelming for users. Additionally, the rag's **reliance on AI algorithms** can lead to biases and errors, which can negatively impact decision-making. Furthermore, the **high cost of implementation** and **maintenance** of the Agentic Rag can be a significant barrier for individuals and organizations with limited resources. Moreover, the **need for continuous updates and training** can be time-consuming and require significant expertise. Finally, the **potential for job displacement** is a significant concern, as the automation of certain tasks can lead to job losses and social unrest.\\n\\n### Real-World Applications of Agentic Rag\\nAgentic Rag has numerous practical applications across various industries. For instance, in the field of education, Agentic Rag can be used to create interactive learning materials that encourage student participation and engagement. In healthcare, it can be utilized to develop personalized treatment plans that take into account individual patient needs and preferences. Additionally, Agentic Rag can be applied in the realm of social media, enabling the creation of dynamic and responsive online content that adapts to user interactions. Other examples of real-world applications include:\\n* **Game development**: Agentic Rag can be used to create immersive and interactive game environments that respond to player actions.\\n* **Virtual reality experiences**: Agentic Rag can enhance virtual reality experiences by allowing for dynamic and adaptive interactions between users and virtual objects.\\n* **Intelligent tutoring systems**: Agentic Rag can be employed to develop intelligent tutoring systems that provide personalized feedback and guidance to learners.\\nThese examples demonstrate the versatility and potential of Agentic Rag in driving innovation and improvement in various fields.\\n\\n### Conclusion and Future Directions\\nIn conclusion, Agentic Rag has emerged as a significant concept, offering insights into the complex relationships between agents, environments, and the dynamic interplay of forces that shape their interactions. Throughout this exploration, we have delved into the foundational aspects of Agentic Rag, its theoretical underpinnings, and its implications for understanding adaptive systems. The main points of discussion have centered around the capacity of agents to perceive their environment, make decisions based on that perception, and act upon those decisions, thereby influencing their surroundings and being influenced by them in return. \\nThe future prospects for Agentic Rag are vast and promising. As research continues to unravel the intricacies of agentic behavior, we can expect significant advancements in fields such as artificial intelligence, where creating agents that can learn, adapt, and interact with their environments in a more human-like manner is a key goal. Moreover, understanding agentic behavior can inform strategies for environmental sustainability, social policy, and technological innovation, by highlighting the importance of considering the dynamic interplay between agents and their environments. \\nLooking ahead, interdisciplinary approaches will be crucial for fully exploring the potential of Agentic Rag. Collaborations between computer scientists, sociologists, psychologists, and environmental scientists, among others, will provide a comprehensive understanding of how agents operate within and influence various types of systems. Additionally, the development of new methodologies and tools for analyzing and modeling agentic behavior will be essential for translating theoretical insights into practical applications. \\nUltimately, the study of Agentic Rag not only deepens our understanding of complex systems but also empowers us to design and manage these systems in ways that are more sustainable, equitable, and beneficial for all agents involved. As we move forward, embracing the complexities and opportunities presented by Agentic Rag will be vital for navigating the challenges and harnessing the potential of our increasingly interconnected world.\\n\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efacddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_out = app.invoke({\"topic\": \"Write a blog on Agentic RAG\", \"sections\":[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99a2ef7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Write a blog on Agentic RAG',\n",
       " 'plan': Plan(blog_title='Understanding Agentic RAG', tasks=[Task(id=1, title='Introduction to Agentic RAG', brief='Define Agentic RAG and its importance'), Task(id=2, title='Key Components of Agentic RAG', brief='Discuss the key components of Agentic RAG'), Task(id=3, title='Benefits of Agentic RAG', brief='Explain the benefits of using Agentic RAG'), Task(id=4, title='Challenges in Implementing Agentic RAG', brief='Discuss the challenges of implementing Agentic RAG'), Task(id=5, title='Real-World Applications of Agentic RAG', brief='Provide examples of real-world applications of Agentic RAG'), Task(id=6, title='Best Practices for Agentic RAG', brief='Offer best practices for implementing and using Agentic RAG'), Task(id=7, title='Conclusion', brief='Summarize the key points and takeaways from the blog')]),\n",
       " 'sections': ['## Introduction to Agentic RAG\\nAgentic RAG, which stands for Retrieval-Augmented Generation, is an artificial intelligence (AI) technology designed to improve the performance and efficiency of large language models. At its core, Agentic RAG combines the strengths of retrieval-based and generation-based approaches to produce more accurate, informative, and contextually relevant text outputs. The importance of Agentic RAG lies in its ability to enhance the capabilities of AI systems in understanding and generating human-like language, thereby facilitating more effective human-computer interactions and paving the way for innovative applications across various industries. By leveraging Agentic RAG, developers can create more sophisticated language models that not only provide better responses to user queries but also continuously learn and improve over time, making them indispensable tools in the realm of natural language processing.',\n",
       "  '### Key Components of Agentic RAG\\nThe Agentic RAG framework comprises several key components that work together to facilitate effective reasoning and decision-making. These components include:\\n* **Retrieval Mechanism**: This component is responsible for fetching relevant information from a large language model or knowledge base, allowing the system to gather context and supporting evidence for its reasoning.\\n* **Generator**: The generator component uses the retrieved information to create a response or answer to a given question or prompt, leveraging the power of large language models to produce coherent and context-specific text.\\n* **Agent**: The agent component acts as the decision-maker, using the generated response as input to make informed decisions or take actions based on the information provided.\\n* **Iteration and Feedback Loop**: This component enables the Agentic RAG system to refine its responses through continuous iteration and feedback, allowing it to learn from its mistakes and adapt to new information or changing contexts.',\n",
       "  '### Benefits of Agentic RAG\\nThe benefits of using Agentic RAG are numerous and can significantly enhance the overall performance of various applications. Some of the key advantages include:\\n* **Improved Information Retrieval**: Agentic RAG enables more accurate and efficient information retrieval by leveraging the strengths of both retrieval-augmented generation (RAG) and agent-based architectures.\\n* **Enhanced Contextual Understanding**: By incorporating agents that can interact with the environment and adapt to changing contexts, Agentic RAG can provide a deeper understanding of the context in which the information is being retrieved.\\n* **Increased Flexibility**: The modular design of Agentic RAG allows for easy integration with various downstream tasks and applications, making it a versatile tool for a wide range of use cases.\\n* **Better Handling of Ambiguity and Uncertainty**: Agentic RAG can effectively handle ambiguous or uncertain queries by using agents to explore different possibilities and provide more informed responses.\\n* **Scalability and Efficiency**: The use of agents in Agentic RAG can help distribute the computational load and improve the overall scalability and efficiency of the system.',\n",
       "  '### Challenges in Implementing Agentic RAG\\nImplementing Agentic RAG, a retrieval-augmented generator, comes with its own set of challenges. One of the primary hurdles is the **need for large-scale datasets** to train the model effectively. Agentic RAG relies on a vast amount of data to learn and improve its generation capabilities, which can be time-consuming and costly to obtain. Additionally, the **complexity of the model architecture** can make it difficult to fine-tune and adjust the parameters for optimal performance. \\nAnother significant challenge is **ensuring the quality and relevance of the retrieved information**. If the retrieval mechanism fails to provide accurate and context-specific information, the overall performance of the Agentic RAG model suffers. \\nFurthermore, **balancing the trade-off between exploration and exploitation** is crucial. The model needs to explore new information to improve its knowledge but also exploit the existing knowledge to generate coherent and relevant text. \\nLastly, **evaluating the performance of Agentic RAG models** can be challenging due to the lack of standardized metrics and benchmarks, making it difficult to compare and improve upon existing models.',\n",
       "  \"### Real-World Applications of Agentic RAG\\nAgentic RAG has numerous real-world applications across various industries, including:\\n* **Healthcare**: Agentic RAG can be used to develop personalized treatment plans for patients, taking into account their unique characteristics, needs, and goals.\\n* **Education**: Agentic RAG can be applied to create adaptive learning systems that tailor instruction to individual students' learning styles, abilities, and interests.\\n* **Customer Service**: Agentic RAG can be used to power chatbots and virtual assistants that provide personalized support and recommendations to customers.\\n* **Marketing**: Agentic RAG can help businesses develop targeted marketing campaigns that resonate with specific audience segments and increase customer engagement.\\n* **Gaming**: Agentic RAG can be used to create immersive and dynamic game environments that respond to players' actions and decisions.\\nThese applications demonstrate the potential of Agentic RAG to drive innovation and improvement in various fields, and its ability to enhance human-machine interaction and collaboration.\",\n",
       "  \"### Best Practices for Agentic RAG\\nTo maximize the potential of Agentic RAG, consider the following best practices for implementation and usage:\\n* **Define Clear Objectives**: Establish specific, measurable, and achievable goals for your Agentic RAG model to ensure it aligns with your project's requirements.\\n* **High-Quality Training Data**: Provide the model with a diverse and extensive dataset that covers various aspects of the task or domain to improve its performance and accuracy.\\n* **Regular Model Updates**: Continuously update and fine-tune the Agentic RAG model with new data and feedback to adapt to changing circumstances and maintain its effectiveness.\\n* **Human Evaluation and Oversight**: Implement a review process to assess the model's outputs and correct any errors or biases, ensuring the results are reliable and trustworthy.\\n* **Transparency and Explainability**: Use techniques to provide insights into the model's decision-making process, enabling better understanding and identification of potential issues.\\n* **Iterative Testing and Refining**: Engage in ongoing testing and refinement of the Agentic RAG model to identify areas for improvement and optimize its overall performance.\",\n",
       "  \"## Conclusion\\nIn conclusion, Agentic RAG is a powerful tool for generating human-like text based on a given prompt. Throughout this blog, we have explored the concept of Agentic RAG, its architecture, and its applications. The key takeaways from this blog include:\\n* Agentic RAG combines the strengths of retrieval-augmented generation and agent-based modeling to produce coherent and context-specific text.\\n* The model's ability to retrieve relevant information from a knowledge base and incorporate it into the generated text makes it a valuable tool for a wide range of natural language processing tasks.\\n* Agentic RAG has the potential to be used in various applications, including chatbots, language translation, and text summarization.\\n* The model's performance can be further improved by fine-tuning it on specific datasets and tasks.\\nOverall, Agentic RAG represents a significant advancement in the field of natural language processing, and its potential applications are vast and exciting.\"],\n",
       " 'final': \"# Understanding Agentic RAG\\n\\n## Introduction to Agentic RAG\\nAgentic RAG, which stands for Retrieval-Augmented Generation, is an artificial intelligence (AI) technology designed to improve the performance and efficiency of large language models. At its core, Agentic RAG combines the strengths of retrieval-based and generation-based approaches to produce more accurate, informative, and contextually relevant text outputs. The importance of Agentic RAG lies in its ability to enhance the capabilities of AI systems in understanding and generating human-like language, thereby facilitating more effective human-computer interactions and paving the way for innovative applications across various industries. By leveraging Agentic RAG, developers can create more sophisticated language models that not only provide better responses to user queries but also continuously learn and improve over time, making them indispensable tools in the realm of natural language processing.\\n\\n### Key Components of Agentic RAG\\nThe Agentic RAG framework comprises several key components that work together to facilitate effective reasoning and decision-making. These components include:\\n* **Retrieval Mechanism**: This component is responsible for fetching relevant information from a large language model or knowledge base, allowing the system to gather context and supporting evidence for its reasoning.\\n* **Generator**: The generator component uses the retrieved information to create a response or answer to a given question or prompt, leveraging the power of large language models to produce coherent and context-specific text.\\n* **Agent**: The agent component acts as the decision-maker, using the generated response as input to make informed decisions or take actions based on the information provided.\\n* **Iteration and Feedback Loop**: This component enables the Agentic RAG system to refine its responses through continuous iteration and feedback, allowing it to learn from its mistakes and adapt to new information or changing contexts.\\n\\n### Benefits of Agentic RAG\\nThe benefits of using Agentic RAG are numerous and can significantly enhance the overall performance of various applications. Some of the key advantages include:\\n* **Improved Information Retrieval**: Agentic RAG enables more accurate and efficient information retrieval by leveraging the strengths of both retrieval-augmented generation (RAG) and agent-based architectures.\\n* **Enhanced Contextual Understanding**: By incorporating agents that can interact with the environment and adapt to changing contexts, Agentic RAG can provide a deeper understanding of the context in which the information is being retrieved.\\n* **Increased Flexibility**: The modular design of Agentic RAG allows for easy integration with various downstream tasks and applications, making it a versatile tool for a wide range of use cases.\\n* **Better Handling of Ambiguity and Uncertainty**: Agentic RAG can effectively handle ambiguous or uncertain queries by using agents to explore different possibilities and provide more informed responses.\\n* **Scalability and Efficiency**: The use of agents in Agentic RAG can help distribute the computational load and improve the overall scalability and efficiency of the system.\\n\\n### Challenges in Implementing Agentic RAG\\nImplementing Agentic RAG, a retrieval-augmented generator, comes with its own set of challenges. One of the primary hurdles is the **need for large-scale datasets** to train the model effectively. Agentic RAG relies on a vast amount of data to learn and improve its generation capabilities, which can be time-consuming and costly to obtain. Additionally, the **complexity of the model architecture** can make it difficult to fine-tune and adjust the parameters for optimal performance. \\nAnother significant challenge is **ensuring the quality and relevance of the retrieved information**. If the retrieval mechanism fails to provide accurate and context-specific information, the overall performance of the Agentic RAG model suffers. \\nFurthermore, **balancing the trade-off between exploration and exploitation** is crucial. The model needs to explore new information to improve its knowledge but also exploit the existing knowledge to generate coherent and relevant text. \\nLastly, **evaluating the performance of Agentic RAG models** can be challenging due to the lack of standardized metrics and benchmarks, making it difficult to compare and improve upon existing models.\\n\\n### Real-World Applications of Agentic RAG\\nAgentic RAG has numerous real-world applications across various industries, including:\\n* **Healthcare**: Agentic RAG can be used to develop personalized treatment plans for patients, taking into account their unique characteristics, needs, and goals.\\n* **Education**: Agentic RAG can be applied to create adaptive learning systems that tailor instruction to individual students' learning styles, abilities, and interests.\\n* **Customer Service**: Agentic RAG can be used to power chatbots and virtual assistants that provide personalized support and recommendations to customers.\\n* **Marketing**: Agentic RAG can help businesses develop targeted marketing campaigns that resonate with specific audience segments and increase customer engagement.\\n* **Gaming**: Agentic RAG can be used to create immersive and dynamic game environments that respond to players' actions and decisions.\\nThese applications demonstrate the potential of Agentic RAG to drive innovation and improvement in various fields, and its ability to enhance human-machine interaction and collaboration.\\n\\n### Best Practices for Agentic RAG\\nTo maximize the potential of Agentic RAG, consider the following best practices for implementation and usage:\\n* **Define Clear Objectives**: Establish specific, measurable, and achievable goals for your Agentic RAG model to ensure it aligns with your project's requirements.\\n* **High-Quality Training Data**: Provide the model with a diverse and extensive dataset that covers various aspects of the task or domain to improve its performance and accuracy.\\n* **Regular Model Updates**: Continuously update and fine-tune the Agentic RAG model with new data and feedback to adapt to changing circumstances and maintain its effectiveness.\\n* **Human Evaluation and Oversight**: Implement a review process to assess the model's outputs and correct any errors or biases, ensuring the results are reliable and trustworthy.\\n* **Transparency and Explainability**: Use techniques to provide insights into the model's decision-making process, enabling better understanding and identification of potential issues.\\n* **Iterative Testing and Refining**: Engage in ongoing testing and refinement of the Agentic RAG model to identify areas for improvement and optimize its overall performance.\\n\\n## Conclusion\\nIn conclusion, Agentic RAG is a powerful tool for generating human-like text based on a given prompt. Throughout this blog, we have explored the concept of Agentic RAG, its architecture, and its applications. The key takeaways from this blog include:\\n* Agentic RAG combines the strengths of retrieval-augmented generation and agent-based modeling to produce coherent and context-specific text.\\n* The model's ability to retrieve relevant information from a knowledge base and incorporate it into the generated text makes it a valuable tool for a wide range of natural language processing tasks.\\n* Agentic RAG has the potential to be used in various applications, including chatbots, language translation, and text summarization.\\n* The model's performance can be further improved by fine-tuning it on specific datasets and tasks.\\nOverall, Agentic RAG represents a significant advancement in the field of natural language processing, and its potential applications are vast and exciting.\\n\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faafde53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog-writing-agent (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
